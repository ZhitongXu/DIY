{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取及预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words spoken: ['eight', 'five', 'four', 'nine', 'one', 'seven', 'six', 'three', 'two', 'zero']\n"
     ]
    }
   ],
   "source": [
    "fpaths = []\n",
    "labels = []\n",
    "spoken = []\n",
    "for f in os.listdir('Audio'):\n",
    "    for w in os.listdir('Audio/' + f):\n",
    "        fpaths.append('Audio/' + f + '/' + w)\n",
    "        labels.append(f)\n",
    "        if f not in spoken:\n",
    "            spoken.append(f)\n",
    "print('Words spoken:', spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files total: 160\n",
      "Labels and label indices [7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 9. 9. 9. 9. 9. 9. 9. 9.\n",
      " 9. 9. 9. 9. 9. 9. 9. 9. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "data = np.zeros((len(fpaths), 70000))\n",
    "maxsize = -1\n",
    "for n,file in enumerate(fpaths):\n",
    "    _, d = wavfile.read(file)\n",
    "    # print(d.shape)\n",
    "    data[n, :d.shape[0]] = d\n",
    "    if d.shape[0] > maxsize:\n",
    "        maxsize = d.shape[0]\n",
    "data = data[:, :maxsize]\n",
    "\n",
    "#Each sample file is one row in data, and has one entry in labels\n",
    "print('Number of files total:', data.shape[0])\n",
    "all_labels = np.zeros(data.shape[0])\n",
    "for n, l in enumerate(set(labels)):\n",
    "    all_labels[np.array([i for i, _ in enumerate(labels) if _ == l])] = n\n",
    "    \n",
    "print('Labels and label indices', all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rhodia\\Anaconda3\\lib\\site-packages\\mkl_fft\\_numpy_fft.py:331: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  output = mkl_fft.rfft_numpy(a, n=n, axis=axis)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13, 382)\n"
     ]
    }
   ],
   "source": [
    "import python_speech_features as speech\n",
    "import librosa\n",
    "all_obs = np.zeros([160, 13, 382])\n",
    "for n,file in enumerate(fpaths):\n",
    "    all_obs[n,]=speech.mfcc(data[n,:]).T\n",
    "print(all_obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13, 382)\n",
      "(160, 61251)\n"
     ]
    }
   ],
   "source": [
    "print(all_obs.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ContinuousHMM实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "\n",
    "class gmmhmm:\n",
    "    def __init__(self, n_states):\n",
    "        self.n_states = n_states\n",
    "        self.random_state = np.random.RandomState(0)\n",
    "        \n",
    "        #Normalize random initial state\n",
    "        self.prior = self._normalize(self.random_state.rand(self.n_states, 1))\n",
    "        self.A = self._stochasticize(self.random_state.rand(self.n_states, self.n_states))\n",
    "        \n",
    "        self.mu = None\n",
    "        self.covs = None\n",
    "        self.n_dims = None\n",
    "           \n",
    "    def _forward(self, B):\n",
    "        log_likelihood = 0.\n",
    "        T = B.shape[1]\n",
    "        alpha = np.zeros(B.shape)\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                alpha[:, t] = B[:, t] * self.prior.ravel()\n",
    "            else:\n",
    "                alpha[:, t] = B[:, t] * np.dot(self.A.T, alpha[:, t - 1])\n",
    "         \n",
    "            alpha_sum = np.sum(alpha[:, t])\n",
    "            alpha[:, t] /= alpha_sum\n",
    "            log_likelihood = log_likelihood + np.log(alpha_sum)\n",
    "        return log_likelihood, alpha\n",
    "    \n",
    "    def _backward(self, B):\n",
    "        T = B.shape[1]\n",
    "        beta = np.zeros(B.shape);\n",
    "           \n",
    "        beta[:, -1] = np.ones(B.shape[0])\n",
    "            \n",
    "        for t in range(T - 1)[::-1]:\n",
    "            beta[:, t] = np.dot(self.A, (B[:, t + 1] * beta[:, t + 1]))\n",
    "            beta[:, t] /= np.sum(beta[:, t])\n",
    "        return beta\n",
    "    \n",
    "    def _state_likelihood(self, obs):\n",
    "        obs = np.atleast_2d(obs)\n",
    "        B = np.zeros((self.n_states, obs.shape[1]))\n",
    "        for s in range(self.n_states):\n",
    "            np.random.seed(self.random_state.randint(1))\n",
    "            B[s, :] = st.multivariate_normal.pdf(\n",
    "                obs.T, mean=self.mu[:, s].T, cov=self.covs[:, :, s].T)\n",
    "        return B\n",
    "    \n",
    "    def _normalize(self, x):\n",
    "        return (x + (x == 0)) / np.sum(x)\n",
    "    \n",
    "    def _stochasticize(self, x):\n",
    "        return (x + (x == 0)) / np.sum(x, axis=1)\n",
    "    \n",
    "    def _em_init(self, obs):\n",
    "        if self.n_dims is None:\n",
    "            self.n_dims = obs.shape[0]\n",
    "        if self.mu is None:\n",
    "            subset = self.random_state.choice(np.arange(self.n_dims), size=self.n_states, replace=False)\n",
    "            self.mu = obs[:, subset]\n",
    "        if self.covs is None:\n",
    "            self.covs = np.zeros((self.n_dims, self.n_dims, self.n_states))\n",
    "            self.covs += np.diag(np.diag(np.cov(obs)))[:, :, None]\n",
    "        return self\n",
    "    \n",
    "    def _em_step(self, obs): \n",
    "        obs = np.atleast_2d(obs)\n",
    "        B = self._state_likelihood(obs)\n",
    "        T = obs.shape[1]\n",
    "        \n",
    "        log_likelihood, alpha = self._forward(B)\n",
    "        beta = self._backward(B)\n",
    "        \n",
    "        xi_sum = np.zeros((self.n_states, self.n_states))\n",
    "        gamma = np.zeros((self.n_states, T))\n",
    "        \n",
    "        for t in range(T - 1):\n",
    "            partial_sum = self.A * np.dot(alpha[:, t], (beta[:, t] * B[:, t + 1]).T)\n",
    "            xi_sum += self._normalize(partial_sum)\n",
    "            partial_g = alpha[:, t] * beta[:, t]\n",
    "            gamma[:, t] = self._normalize(partial_g)\n",
    "              \n",
    "        partial_g = alpha[:, -1] * beta[:, -1]\n",
    "        gamma[:, -1] = self._normalize(partial_g)\n",
    "        \n",
    "        expected_prior = gamma[:, 0]\n",
    "        expected_A = self._stochasticize(xi_sum)\n",
    "        \n",
    "        expected_mu = np.zeros((self.n_dims, self.n_states))\n",
    "        expected_covs = np.zeros((self.n_dims, self.n_dims, self.n_states))\n",
    "        \n",
    "        gamma_state_sum = np.sum(gamma, axis=1)\n",
    "        #Set zeros to 1 before dividing\n",
    "        gamma_state_sum = gamma_state_sum + (gamma_state_sum == 1)\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            gamma_obs = obs * gamma[s, :]\n",
    "            expected_mu[:, s] = np.sum(gamma_obs, axis=1) / gamma_state_sum[s]\n",
    "            partial_covs = np.dot(gamma_obs, obs.T) / gamma_state_sum[s] - np.dot(expected_mu[:, s], expected_mu[:, s].T)\n",
    "            #Symmetrize\n",
    "            partial_covs = np.triu(partial_covs) + np.triu(partial_covs).T - np.diag(partial_covs)\n",
    "        \n",
    "        expected_covs += .01 * np.eye(self.n_dims)[:, :, None]\n",
    "        \n",
    "        self.prior = expected_prior\n",
    "        self.mu = expected_mu\n",
    "        self.covs = expected_covs\n",
    "        self.A = expected_A\n",
    "        return log_likelihood\n",
    "    \n",
    "    def fit(self, obs, n_iter=8):\n",
    "        #3D should be n_examples, n_features, n_dims\n",
    "        #For example, with 6 features per speech segment, 105 different words\n",
    "        #this array should be size\n",
    "        #(105, 6, X) where X is the number of frames with features extracted\n",
    "        count = obs.shape[0]\n",
    "        for n in range(count):\n",
    "            for i in range(n_iter):\n",
    "                self._em_init(obs[n, :, :])\n",
    "                log_likelihood = self._em_step(obs[n, :, :])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, obs):\n",
    "        count = obs.shape[0]\n",
    "        out = np.zeros((count,))\n",
    "        for n in range(count):\n",
    "            B = self._state_likelihood(obs[n, :, :])\n",
    "            log_likelihood, _ = self._forward(B)\n",
    "            out[n] = log_likelihood\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分训练测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix: (144, 13, 382)\n",
      "Size of testing matrix: (16, 13, 382)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(all_labels, test_size=0.1, random_state=0)\n",
    "\n",
    "for n,i in enumerate(all_obs):\n",
    "    all_obs[n] /= all_obs[n].sum(axis=0)\n",
    "\n",
    "for train_index, test_index in sss:\n",
    "    X_train, X_test = all_obs[train_index, ...], all_obs[test_index, ...]\n",
    "    y_train, y_test = all_labels[train_index], all_labels[test_index]\n",
    "print('Size of training matrix:', X_train.shape)\n",
    "print('Size of testing matrix:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rhodia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Rhodia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Rhodia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 6.25 percent\n"
     ]
    }
   ],
   "source": [
    "ys = set(all_labels)\n",
    "ms = [gmmhmm(6) for y in ys]\n",
    "_ = [m.fit(X_train[y_train == y, :, :]) for m, y in zip(ms, ys)]\n",
    "ps = [m.transform(X_test) for m in ms]\n",
    "res = np.vstack(ps)\n",
    "predicted_labels = np.argmax(res, axis=0)\n",
    "\n",
    "missed = (predicted_labels != y_test)\n",
    "print('Test accuracy: %.2f percent' % (100 * (1 - np.mean(missed))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
